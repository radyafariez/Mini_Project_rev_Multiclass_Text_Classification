# -*- coding: utf-8 -*-
"""[Mini_Project-rev]_Multiclass_Text_Classification_Mohammad_Radya_Fariez

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mg6ghjRReC3QA6K0-cQMwc23lztWwaK
"""

import pandas as pd

# install kaggle package
!pip install -q kaggle

from google.colab import files
files.upload()

dataf = pd.read_csv('bbc-news-data.csv', sep='\t')
dataf.head()

dataf.info()

dataf.shape

dataf_new = dataf.drop(columns=['filename'])
dataf_new

import nltk, os, re, string

import tensorflow as tf
from tensorflow import keras
from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence

from keras.utils import to_categorical

from keras_preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')

# lower-case all characters
dataf_new.title = dataf_new.title.apply(lambda x: x.lower())
dataf_new.content = dataf_new.content.apply(lambda x: x.lower())

# removing functuation
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    dataf_new.title = dataf_new.title.apply(lambda x: cleaner(x))
    dataf_new.content = dataf_new.content.apply(lambda x: lem(x))

## lematization
lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    dataf_new.title = dataf_new.title.apply(lambda x: lem(x))
    dataf_new.content = dataf_new.content.apply(lambda x: lem(x))

# removing number
def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    dataf_new['title'].apply(rem_numbers)
    dataf_new['content'].apply(rem_numbers)

# removing stopword
st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    dataf_new.title = dataf_new.title.apply(lambda x: stopword(x))
    dataf_new.content = dataf_new.content.apply(lambda x: lem(x))

#new data view after cleansing
dataf_new.head(11)

#Categorical -> One Hot Encoding
category = pd.get_dummies(dataf_new.category)
dataf_new_cat = pd.concat([dataf_new, category], axis=1)
dataf_new_cat = dataf_new_cat.drop(columns='category')
dataf_new_cat

#Dataframe -> Numpy Array
news = dataf_new_cat['title'].values + '' + dataf_new_cat['content'].values
label = dataf_new_cat[['business', 'entertainment', 'politics', 'sport', 'tech']].values

label

#Train test Split
from sklearn.model_selection import train_test_split
news_train, news_test, label_train, label_test = train_test_split(news, label, test_size=0.2, shuffle = True)

#Kata pada dataset -> bilangan numerik menggunakan fungsi tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(news_train) 
tokenizer.fit_on_texts(news_test)
 
sekuens_train = tokenizer.texts_to_sequences(news_train)
sekuens_test = tokenizer.texts_to_sequences(news_test)
 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    #tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

pd.DataFrame(padded_train).max()

#Callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.8 and logs.get('val_accuracy')>0.8):
      self.model.stop_training = True
      print("\nThe Accuracy of the Training and Validation set has reached > 80%!")
callbacks = myCallback()

#Model Train
history = model.fit(padded_train, label_train, epochs= 50, 
                    validation_data=(padded_test, label_test), verbose=2, callbacks = [callbacks], validation_steps=30)

